# Smart Toddler Care Assistant (1–3 Years) RAG System

## 1. Main Idea

The Smart Toddler Care Assistant (1–3 Years) is an AI-powered question–answering system based on the Retrieval-Augmented Generation (RAG) approach. The goal of the project is to support parents of toddlers by providing reliable, scientifically grounded, and developmentally appropriate information about child health, nutrition, daily routines, development, Montessori-based learning, and play activities.

This system retrieves information from a curated knowledge base of trusted pediatric and educational sources. The assistant is designed for educational and advisory purposes and does not replace professional medical consultation.

---

## 2. Key Concepts

The system is based on the concepts of Retrieval-Augmented Generation, semantic embeddings, vector similarity search, document chunking, local AI inference, prompt engineering, and interactive human–AI communication.

---

## 3. Design details

- The system is a local RAG-based application for toddler care questions.

- The user interacts with the system through a Streamlit web interface.

- The user query is first expanded using an LLM to improve semantic search quality.

- The expanded query is converted into a vector embedding using EmbeddingGemma model.

- Dataset documents are cleaned, chunked, embedded, and stored in a Weaviate vector database.

- Relevant text chunks are retrieved using vector similarity search with cosine distance.

- The retrieved context and the original query are combined into a single prompt.

- A local Google Gemma LLM generates the final answer based only on the retrieved data.

---

## 4. Dataset

The dataset is a small, domain-specific knowledge base created for testing a RAG system. It includes three articles on toddler health, daily routines, and Montessori activities for children aged 1–3 years. The content was generated by GPT based on general knowledge inspired by sources such as WHO, CDC, NHS, and Montessori education principles.

---

## 5. System technicl details

The system architecture consists of the following components:

1. User Interface (UI)
 Streamlit web application is used to create a simple local web interface where users can enter questions and view AI-generated responses

2. Embedding Model
 EmbeddingGemma model converts text into numerical vectors

3. Vector Database
Weaviate stores document embeddings and enables fast vector-based retrieval of the most relevant text chunks for a given query.

4. Retrieval Module
The retrieval module is implemented using Weaviate vector database with cosine similarity for semantic search. Query and document embeddings are generated using the EmbeddingGemma model and integrated through LangChain.

5. LLM Client
Google Gemma LLM model is used to generate the final answer using the original question combined with the retrieved context.

6. RAG Pipeline
The retrieved documents and user query are combined into a single prompt and passed to the LLM to produce an accurate, grounded response.

---

## 6. Requirements

weaviate-client==4.18.1
langchain~=0.3.0
langchain-openai~=0.2.0
huggingface-hub==0.36.0
python-dotenv~=1.0.0
pandas~=2.2.0
sentence-transformers==5.1.2
accelerate==1.12.0
streamlit

---

## 7. Limitations

- Small dataset limits coverage and depth
- CPU-only local inference may be slow for large LLMs
- Answers are restricted to dataset content
- Dataset must be manually updated; no automatic web updates

---

## 8. Link to video and Github

- https://github.com/Tomiris95/epam_train/tree/main/Module%203
- https://youtu.be/tmXx-3Oc8Pk

---

